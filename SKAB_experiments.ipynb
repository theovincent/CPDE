{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (for colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/theovincent/CPDE.git -b make_ipynb_working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"/content/CPDE\")\n",
    "\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import ruptures as rpt\n",
    "from SKAB_data.evaluating import evaluating_change_point\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ensemble_methods.aggregations import SCALING_AGGREGATION\n",
    "\n",
    "SINGLE_COSTS = (\n",
    "    {'name': 'ar_1', 'cost':'ar', 'params':{'order':1}},\n",
    "    {'name': 'mahalanobis', 'cost':'mahalanobis', 'params':{}},\n",
    "    {'name': 'l1', 'cost':'l1', 'params':{}},\n",
    "    {'name': 'l2', 'cost':'l2', 'params':{}},\n",
    "    {'name': 'linear', 'cost':'linear', 'params':{}},\n",
    "    {'name': 'rbf', 'cost': 'rbf', 'params': {}}\n",
    ")\n",
    "LIST_COSTS = [dict_cost[\"cost\"] for dict_cost in SINGLE_COSTS]\n",
    "PARAMS = {\"ar\": {'order':1}}\n",
    "\n",
    "DESIRED_ORDER = [\"Standart\", \"LowFP\", \"LowFN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# benchmark files checking\n",
    "all_files=[]\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"SKAB_data/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "             all_files.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "# datasets with anomalies loading\n",
    "test = [pd.read_csv(file, \n",
    "                          sep=';', \n",
    "                          index_col='datetime', \n",
    "                          parse_dates=True).drop('anomaly', axis=1) for file in all_files if 'anomaly-free' not in file]\n",
    "\n",
    "# Save labels\n",
    "true_cp = [dataset.changepoint for dataset in test]\n",
    "\n",
    "# Standardise\n",
    "for idx_data in range(len(test)):\n",
    "    stsc = StandardScaler()\n",
    "    test[idx_data] = pd.DataFrame(stsc.fit_transform(test[idx_data].drop(\"changepoint\", axis=1)), columns=test[idx_data].columns.drop(\"changepoint\"), index=test[idx_data].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_signal = 3\n",
    "\n",
    "break_points = list(np.nonzero(true_cp[idx_signal].values == 1)[0]) + [test[idx_signal].shape[0]]\n",
    "_ = rpt.display(test[idx_signal].values, break_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_search(cost, params, **kwargs):\n",
    "    predicted_cp = []\n",
    "    for idx_data in range(len(test)):\n",
    "        algo = rpt.Window(model=cost, \n",
    "                          params=params, \n",
    "                          width=20,\n",
    "                          jump=1)\n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "        \n",
    "        single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "table_costs_window = {}\n",
    "for cost in tqdm(SINGLE_COSTS):\n",
    "        table_costs_window[cost[\"name\"]] = window_search(**cost)\n",
    "\n",
    "pd.DataFrame(table_costs_window).T[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_search_ensemble_bound():\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test))):\n",
    "        best_nab_sum = - float(\"inf\")\n",
    "        best_single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "\n",
    "        for model in SINGLE_COSTS:\n",
    "            algo = rpt.Window(model=model[\"cost\"], \n",
    "                            params=model[\"params\"], \n",
    "                            width=20,\n",
    "                            jump=1)\n",
    "            algo.fit(np.array(test[idx_data]))\n",
    "            my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "            \n",
    "            single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "            single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "            \n",
    "            nab_model = evaluating_change_point([true_cp[idx_data]], [single_predicted_cp], metric='nab', numenta_time='30 sec')\n",
    "            \n",
    "            if sum(list(nab_model.values())) > best_nab_sum:\n",
    "                best_nab_sum = sum(list(nab_model.values()))\n",
    "                best_single_predicted_cp = single_predicted_cp\n",
    "                \n",
    "        predicted_cp.append(best_single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "window_ensemble_bound = window_search_ensemble_bound()\n",
    "pd.DataFrame(window_ensemble_bound, index=[\"Ensemble bound\"])[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_methods.window_ensemble import WindowEnsemble\n",
    "\n",
    "def window_search_ensemble(scale_aggregation):\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test)), leave=False, position=1):\n",
    "        algo = WindowEnsemble(\n",
    "            width=20,\n",
    "            models=LIST_COSTS,\n",
    "            params=PARAMS, \n",
    "            scale_aggregation=scale_aggregation,\n",
    "            jump=1\n",
    "        )\n",
    "        single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "        \n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=1)\n",
    "\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ensemble_window = {}\n",
    "for scale_aggregation_name, scale_aggregation in tqdm(SCALING_AGGREGATION.items(), position=0):\n",
    "    table_ensemble_window[scale_aggregation_name] = window_search_ensemble(scale_aggregation)\n",
    "\n",
    "pd.DataFrame(table_ensemble_window).T[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynp_search(cost, params, **kwargs):\n",
    "    predicted_cp = []\n",
    "    for idx_data in range(len(test)):\n",
    "        algo = rpt.Dynp(model=cost, \n",
    "                          params=params,\n",
    "                          jump=1)\n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "        \n",
    "        single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "table_costs_dynp = {}\n",
    "for cost in tqdm(SINGLE_COSTS):\n",
    "    table_costs_dynp[cost[\"name\"]] = dynp_search(**cost)\n",
    "\n",
    "pd.DataFrame(table_costs_dynp).T[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynp_search_ensemble_bound():\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test))):\n",
    "        best_nab_sum = - float(\"inf\")\n",
    "        best_single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "\n",
    "        for model in SINGLE_COSTS:\n",
    "            algo = rpt.Dynp(model=model[\"cost\"], \n",
    "                          params=model[\"params\"],\n",
    "                          jump=1)\n",
    "            algo.fit(np.array(test[idx_data]))\n",
    "            my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "            \n",
    "            single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "            single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "            \n",
    "            nab_model = evaluating_change_point([true_cp[idx_data]], [single_predicted_cp], metric='nab', numenta_time='30 sec')\n",
    "            \n",
    "            if sum(list(nab_model.values())) > best_nab_sum:\n",
    "                best_nab_sum = sum(list(nab_model.values()))\n",
    "                best_single_predicted_cp = single_predicted_cp\n",
    "                \n",
    "        predicted_cp.append(best_single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "dynp_ensemble_bound = dynp_search_ensemble_bound()\n",
    "pd.DataFrame(dynp_ensemble_bound, index=[\"Ensemble bound\"])[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_methods.dynamic_programming_ensemble import DynpEnsemble\n",
    "\n",
    "def dynamique_programming_ensemble(scale_aggregation):\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test)), leave=False, position=1):\n",
    "        algo = DynpEnsemble(\n",
    "            models=LIST_COSTS,\n",
    "            params=PARAMS, \n",
    "            scale_aggregation=scale_aggregation,\n",
    "            jump=1\n",
    "        )\n",
    "        single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "        \n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=1)\n",
    "\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ensemble_dynp = {}\n",
    "for scale_aggregation_name, scale_aggregation in tqdm(SCALING_AGGREGATION.items(), position=0):\n",
    "    table_ensemble_dynp[scale_aggregation_name] = dynamique_programming_ensemble(scale_aggregation)\n",
    "\n",
    "pd.DataFrame(table_ensemble_dynp).T[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binseg_search(cost, params, **kwargs):\n",
    "    predicted_cp = []\n",
    "    for idx_data in range(len(test)):\n",
    "        algo = rpt.Binseg(model=cost, \n",
    "                          params=params,\n",
    "                          jump=1)\n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "        \n",
    "        single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "table_costs_dynp = {}\n",
    "for cost in tqdm(SINGLE_COSTS):\n",
    "    table_costs_dynp[cost[\"name\"]] = binseg_search(**cost)\n",
    "\n",
    "pd.DataFrame(table_costs_dynp).T[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binseg_search_ensemble_bound():\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test))):\n",
    "        best_nab_sum = - float(\"inf\")\n",
    "        best_single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "\n",
    "        for model in SINGLE_COSTS:\n",
    "            algo = rpt.Binseg(model=model[\"cost\"], \n",
    "                          params=model[\"params\"],\n",
    "                          jump=1)\n",
    "            algo.fit(np.array(test[idx_data]))\n",
    "            my_bkps = algo.predict(n_bkps=sum(true_cp[idx_data] == 1))\n",
    "            \n",
    "            single_predicted_cp = pd.Series(data=0, index=test[idx_data].index)\n",
    "            single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "            \n",
    "            nab_model = evaluating_change_point([true_cp[idx_data]], [single_predicted_cp], metric='nab', numenta_time='30 sec')\n",
    "            \n",
    "            if sum(list(nab_model.values())) > best_nab_sum:\n",
    "                best_nab_sum = sum(list(nab_model.values()))\n",
    "                best_single_predicted_cp = single_predicted_cp\n",
    "                \n",
    "        predicted_cp.append(best_single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab\n",
    "\n",
    "binseg_ensemble_bound = binseg_search_ensemble_bound()\n",
    "pd.DataFrame(binseg_ensemble_bound, index=[\"Ensemble bound\"])[DESIRED_ORDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_methods.binary_segmentation_ensemble import BinsegEnsemble\n",
    "\n",
    "def binary_segmentation_ensemble(scale_aggregation):\n",
    "    predicted_cp = []\n",
    "    for idx_data in tqdm(range(len(test)), leave=False, position=1):\n",
    "        algo = BinsegEnsemble(\n",
    "            models=LIST_COSTS,\n",
    "            params=PARAMS, \n",
    "            scale_aggregation=scale_aggregation,\n",
    "            jump=1\n",
    "        )\n",
    "        single_predicted_cp = pd.Series(data=0, index=true_cp[0].index)\n",
    "        \n",
    "        algo.fit(np.array(test[idx_data]))\n",
    "        my_bkps = algo.predict(n_bkps=1)\n",
    "\n",
    "        single_predicted_cp[single_predicted_cp.index[my_bkps[:-1]]] = 1\n",
    "        predicted_cp.append(single_predicted_cp)\n",
    "\n",
    "    nab = evaluating_change_point(true_cp, predicted_cp, metric='nab', numenta_time='30 sec')\n",
    "    return nab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ensemble_dynp = {}\n",
    "for scale_aggregation_name, scale_aggregation in tqdm(SCALING_AGGREGATION.items(), position=0):\n",
    "    table_ensemble_dynp[scale_aggregation_name] = binary_segmentation_ensemble(scale_aggregation)\n",
    "\n",
    "pd.DataFrame(table_ensemble_dynp).T[DESIRED_ORDER]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aaa9ed3599f8e76e96f2ede0232ba237938d872a1ef871a32732cbd7a6f3e73"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
